"""
Main execution file for govSpark document analysis.
Orchestrates the entire pipeline: loading ‚Üí tokenizing ‚Üí embedding ‚Üí clustering ‚Üí visualization
"""

from modules.load_document import load_html_document, validate_document_content
from modules.tokenize import tokenize_into_sentences, get_tokenization_stats
from modules.embed_text import create_embeddings, get_embedding_stats, save_embeddings
from config import DOCUMENT_PATH
import sys


def main():
    """
    Main function that orchestrates the entire document analysis pipeline.
    
    This function calls each module in sequence:
    1. Load the HTML document
    2. Tokenize into sentences
    3. Create embeddings using SBERT
    4. Reduce dimensions (to be implemented)
    5. Cluster sentences (to be implemented)
    6. Visualize clusters (to be implemented)
    7. Summarize results (to be implemented)
    """
    
    print("üöÄ Starting govSpark Document Analysis Pipeline")
    print("=" * 60)
    
    # Step 1: Load the HTML document
    print(f"üìÑ Loading document: {DOCUMENT_PATH}")
    try:
        document_text = load_html_document(DOCUMENT_PATH)
        
        # Validate the loaded content
        if not validate_document_content(document_text):
            print("‚ùå Error: Document content appears to be invalid or too short")
            sys.exit(1)
            
        print(f"‚úÖ Successfully loaded document ({len(document_text):,} characters)")
        
    except Exception as e:
        print(f"‚ùå Error loading document: {e}")
        sys.exit(1)
    
    # Step 2: Tokenize the document into sentences
    print("\nüî§ Tokenizing document into sentences...")
    try:
        sentences = tokenize_into_sentences(document_text)
        
        # Get and display tokenization statistics
        stats = get_tokenization_stats(sentences)
        print(f"‚úÖ Tokenization complete:")
        print(f"   ‚Ä¢ Total sentences: {stats['total_sentences']:,}")
        print(f"   ‚Ä¢ Average length: {stats['avg_length']:.1f} characters")
        print(f"   ‚Ä¢ Length range: {stats['min_length']}-{stats['max_length']} characters")
        
        # Show a few example sentences
        print(f"\nüìù Example sentences:")
        for i, sentence in enumerate(sentences[:3]):
            print(f"   {i+1}. {sentence[:100]}{'...' if len(sentence) > 100 else ''}")
            
    except Exception as e:
        print(f"‚ùå Error during tokenization: {e}")
        sys.exit(1)
    
    # Step 3: Create embeddings using SBERT
    print("\nüß† Creating sentence embeddings using SBERT...")
    try:
        embeddings = create_embeddings(sentences, show_progress=True)
        
        # Get and display embedding statistics
        embed_stats = get_embedding_stats(embeddings)
        print(f"‚úÖ Embeddings created successfully:")
        print(f"   ‚Ä¢ Number of embeddings: {embed_stats['num_embeddings']:,}")
        print(f"   ‚Ä¢ Embedding dimensions: {embed_stats['embedding_dim']}")
        print(f"   ‚Ä¢ Mean norm: {embed_stats['mean_norm']:.3f}")
        print(f"   ‚Ä¢ Memory usage: {embed_stats['memory_mb']:.1f} MB")
        
        # Save embeddings for potential reuse
        save_embeddings(embeddings, "embeddings.npy")
        
    except Exception as e:
        print(f"‚ùå Error during embedding creation: {e}")
        sys.exit(1)
    
    # Placeholder for remaining steps
    print(f"\n‚è≥ Next steps (to be implemented):")
    print(f"   ‚Ä¢ Reduce dimensions with UMAP")
    print(f"   ‚Ä¢ Cluster using HDBSCAN")
    print(f"   ‚Ä¢ Visualize clusters")
    print(f"   ‚Ä¢ Generate cluster summaries")
    
    print(f"\nüéâ Pipeline execution complete!")


if __name__ == "__main__":
    main()